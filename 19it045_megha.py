# -*- coding: utf-8 -*-
"""19IT045_Megha.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IMsGqPKLna0LGmEp58hoj1_4BU7LzFgz
"""

! pip install kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d aimuratzhetkizgenov/skin-diseases

!unzip /content/skin-diseases.zip

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras import regularizers
from tensorflow import keras
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg19 import VGG19
from tensorflow.keras.applications.xception import Xception
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.resnet_v2 import ResNet152V2
from tensorflow.keras.applications.mobilenet import MobileNet
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2
from tensorflow.keras.preprocessing import image
from keras.layers import Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
import tensorflow as tf
import os
from glob import glob
from imutils import paths
from skimage import color
import cv2
from PIL import Image
import tqdm

base_model = ResNet152V2(weights='imagenet',
include_top=False,
input_shape=(224, 224, 3))

print(base_model.summary())

for layer in base_model.layers:
   layer.trainable = False

for i, layer in enumerate(base_model.layers):
   print(i, layer.name, layer.trainable)

train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   rotation_range=0.2,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   vertical_flip=True,
                                   fill_mode='nearest')
train_generator = train_datagen.flow_from_directory('/content/dataset/Train',
                                                    target_size=(224,224),
                                                    batch_size=32,
                                                    class_mode='categorical')

valid_datagen = ImageDataGenerator(rescale=1./255)
valid_generator = valid_datagen.flow_from_directory('/content/dataset/Test',
                                                    target_size=(224,224),
                                                    batch_size=32,
                                                    class_mode='categorical',shuffle=False)

x = Flatten()(base_model.output)
x = Dense(50,activation='relu',kernel_regularizer=regularizers.l2(0.001))(x)
prediction = Dense(9, activation='softmax')(x)


# create a model object
model = Model(inputs=base_model.input, outputs=prediction)
model.summary()

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy','AUC','Precision','Recall'])

print("model compiled")
print(model.summary())

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau
early = [EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto'),ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, 
                                   verbose=1, mode='max', min_lr=0.00001)]
history = model.fit_generator(train_generator,
                              epochs=10,
                              validation_data = valid_generator,
                              callbacks=early
                              )

"""### Fine Tuning




*   Large data but different from Pretrained data (Train the entire model)
*   Large data but same as Pretrained data (Train some layers and freeze others)
*   Small data but different from Pretrained data (Train some layers and freeze others)
*   Small data but same as Pretrained data (Freeze the convolutional base)

In our case Dataset is not so large so we will use 3rd technique. Hence we will unfreeze some last layer and then we will train our model on that layer except that unfreeze layer we will freeze all other layers.





"""

for layer in base_model.layers[:550]:
   layer.trainable = False
for layer in base_model.layers[550:]:
   layer.trainable = True

for i, layer in enumerate(base_model.layers):
   print(i, layer.name, layer.trainable)

x = Flatten()(base_model.output)
prediction = Dense(9, activation='softmax')(x)


# create a model object
model = Model(inputs=base_model.input, outputs=prediction)
model.summary()

#Now we will train our last unfreeze layer small low learning rate
model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy','AUC','Precision','Recall'])

print("model compiled")
print(model.summary())

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
early = [EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto'),ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, 
                                   verbose=1, mode='max', min_lr=0.000001)]
history = model.fit_generator(train_generator,
                              epochs=30,
                              validation_data = valid_generator,
                              callbacks=early
                              )

Here i have got 48.31% accuracy .